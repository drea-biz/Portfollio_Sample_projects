{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio \n",
    "## Andrea Mejia \n",
    "#### (858) 466-9040 \n",
    "#### Email me at betterbiz.drea@gmail.com\n",
    "This notebook to demonstrate some of my skillsets in python. This Project will focus on demonstrating data mining, web scraping, data analytics and basic machine learning. \n",
    "\n",
    "I will be demonstrating an analysis of twitter post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/drea/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "#import that social networking service scraper\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape\n",
    "# Import nltk package \n",
    "# NLTK provides support for a wide variety of text processing tasks: \n",
    "# tokenization, stemming, proper name identification, part of speech identification, etc. \n",
    "#   PennTreeBank word tokenizer \n",
    "#   English language stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon') # this will help us analyze tweet emotions\n",
    "# scikit-learn imports\n",
    "#   TF-IDF Vectorizer that first removes widely used words in the dataset and then transforms test data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# snsscrape will scrape twitter\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import snscrape as sn\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "# import re for regular expression\n",
    "import re\n",
    "\n",
    "# seaborn for plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2, style=\"white\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# set plotting size parameter\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# improve resolution\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/drea/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/drea/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords & punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "query = str\n",
    "def twitter_search(query):\n",
    "    \"\"\" This function provides a convient way to retrieve twitter data and put it in a dataframe\"\"\"\n",
    "\n",
    "    query = query\n",
    "    tweets = []\n",
    "    limit = 10000\n",
    "\n",
    "\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        \n",
    "        if len(tweets) == limit: \n",
    "            break\n",
    "        else:\n",
    "            tweets.append([tweet.date,tweet.user.username,tweet.content,])\n",
    "        \n",
    "    df = pd.DataFrame(tweets, columns = ['Date','User','content'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-24 22:30:28+00:00</td>\n",
       "      <td>Padres</td>\n",
       "      <td>5 years ago, @THoffman51 got the HOF call... h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-24 22:18:59+00:00</td>\n",
       "      <td>ColeThomas0</td>\n",
       "      <td>a zillion/10 @Padres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-24 21:44:16+00:00</td>\n",
       "      <td>hector8c</td>\n",
       "      <td>One month later, still hurts @Padres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-24 21:27:57+00:00</td>\n",
       "      <td>10NewsCoronel</td>\n",
       "      <td>.@MLB investigating @WhiteSox pitcher &amp;amp; fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-24 21:19:56+00:00</td>\n",
       "      <td>TheKingSource</td>\n",
       "      <td>Yes and zero offers. You'd think a time whiteo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date           User  \\\n",
       "0 2023-01-24 22:30:28+00:00         Padres   \n",
       "1 2023-01-24 22:18:59+00:00    ColeThomas0   \n",
       "2 2023-01-24 21:44:16+00:00       hector8c   \n",
       "3 2023-01-24 21:27:57+00:00  10NewsCoronel   \n",
       "4 2023-01-24 21:19:56+00:00  TheKingSource   \n",
       "\n",
       "                                             content  \n",
       "0  5 years ago, @THoffman51 got the HOF call... h...  \n",
       "1                               a zillion/10 @Padres  \n",
       "2               One month later, still hurts @Padres  \n",
       "3  .@MLB investigating @WhiteSox pitcher &amp; fo...  \n",
       "4  Yes and zero offers. You'd think a time whiteo...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = twitter_search(' (@padres) until:2023-12-31 since:2023-01-01 -filter:replies')\n",
    "test_df_copy = test_df.copy() #copy of test_df for sentiment analysis\n",
    "test_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's download it so we don't have to run the cell everytime. \n",
    "test_df_copy.to_csv('Test_Content_dataset', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
